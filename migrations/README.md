Search and Data Migrations
==========================
This module contains search index migrations (which add, delete, or change the contents of search documents on our App Engine search index) and data migrations (which add, delete, or update data in our Datastore). Search index migrations take place on our App Engine standard default service via our `/api/migrations` endpoint. Data migrations take place on an App Engine flex instance which builds an Apache Beam pipeline and sends it to Google's Dataflow. Apache beam cannot run on App Engine standard (it uses C libraries), so it must be run on the flex service instead. Search index migrations cannot take place on dataflow since App Engine search isn't considered a valid source/sink for a pipeline. Therefore, we decided to keep the two functionalities separate.


Search Migrations
------------------
These live in `migrations/tasks/` and are run using Google's now deprecated map reduce method. The bulk of our search index migration map reduce code lives in `util/migrations.py`. Our map reduce code used to also run our data migrations, but we wanted to move as many things as possible to Dataflow, which is the supported method for data migrations. 


Data Migrations
----------------

### Architecture
A Flask app is deployed to an App Engine flex service, which uses Gunicorn to run the app. Our migration endpoint (`/api/migrations`) is still the place to go to kick off all data and search index migrations. Now, however, `migrations/serializer.py` funnels requests for data migrations through to the flex service's Flask app. The Flask app creates a `MigrationHistory` object and kicks off the data migration using a Celery + RabbitMQ task queue. While the migration runs, the Flask app returns a response, and the user is shown their `MigrationHistory` status at the `/api/migrations` endpoint (just the way it was done before). Meanwhile, the migration serializer (back on our standard service) kicks off a deferred App Engine task to monitor the status of the `MigrationHistory` object created by the flex service. Our flex service's Celery task has constructed the Apache Beam pipeline and sent it off to Dataflow at this point. It waits for the pipeline to return the job status and uses it to update the `MigrationHistory` object. Once the status of the `MigrationHistory` object shows that it's successful, the deferred task (back on our standard service) kicks off any post-migration work for this particular migration. This usually involves updating search documents to reflect the migrated data.

We use supervisord to kick off the Gunicorn, RabbitMQ, and Celery processes when the flex service's Docker container boots. When a task queue receives a migration name from the Flask app, it calls the appropriate Apache Beam pipeline, which is built and then sent to a Dataflow worker. The worker does the heavy lifting of actually running the migration and reading/writing from Datastore.

### App Engine Flex Environment Infrastructure
The infrastructure underlying data migrations is complex. It is packaged as a Flask app which is deployed to a Docker container in an App Engine flex instance. We also run Celery + RabbitMQ on the flex instance to handle task queues, so supervisord is required to kick off multiple processes in the Docker container. The following files are a part of this infrastructure:

- *Dockerfile*: Loads Python2.7 Docker image; Sets up supervisord, Celery, and RabbitMQ; Installs app requirements; Calls supervisord to kick off Gunicorn, RabbitMQ, and Celery.
- *appengine_config.py*: A bootstrapping hook that exists on every App Engine instance; This particular one tells App Engine there are multiple libs named `google` that need to be appended to the Pythonpath.
- *celeryd*: The Celery configuration file; Tells Celery what directory to work in, where the Celery tasks live, how many workers to run, when to time out workers, what serialization method to use (always use 'pickle'), and who its user is.
- *data_migrations_app.yaml*: Configuration for our App Engine instances; Tells App Engine how many instances to run, what resources to throw at them, how to scale, what environment and runtime we're using.
- *gunicorn.conf.py*: Gunicorn config; Tells Gunicorn what port to bind to, how many workers to use.
- *main.py*: When Gunicorn starts up, the entrypoint is this script; It imports our Flask app from the `dataflow_migrations/` app directory.
- *requirements.txt*: The requirements that the Dockerfile pip installs on our App Engine flex instance; These requirements are installed in the Docker container.
- *setup.py*: Our data migration code is spread over multiple scripts. `setup.py` is used to package all relevant scripts when our data migration pipelines are sent from the flex instance to Dataflow. Otherwise, when the pipelines attempt to run on Dataflow, they get import errors. `setup.py` must stay in the `migrations/` directory to capture all relevant package code. Apache Beam allows one to set a flag on each pipeline that pickles the current Python session (all the libs and scripts currently loaded by the interpreter) and sends everything to Dataflow. This flag bit us because the current Python session on the flex instance includes our Flask App, Gunicorn, etc. All libs that are not installed on Dataflow workers. So we use the `setup.py` file to package only our data migration code and send that over instead.
- *supervisord.conf*: Runs Celery, RabbitMQ, and Gunicorn processes in the Docker container when it boots. This is necessary to run more than one process on boot. This also gives us more granular control over where logs go, how many times to restart services, etc.
- *dispatch.yaml*: *THIS LIVES IN THE ROOT DIRECTORY*; Tells App Engine about our `migrations-flex` service.

### The `dataflow_migrations` App
This code is the data migrations app logic. It contains the Flask app, our Apache Beam pipelines, pipeline base classes, and helper functions. Note that *no code inside dataflow_migrations/ should reference code from outside that directory*. The code lives on its own service and doesn't know of the outer code's existence. However, the outer code, which lives on App Engine standard, _can_ see code that lives in `dataflow_migrations/`, so it can grab code from that inner directory. All imports inside `dataflow_migrations/` are relative for these reasons. Otherwise we get nasty import errors.

- *api.py*: The flask app that `migrations/serializer.py` passes migration requests through to the Celery task queues. It ensures requests only come from our App Engine project URLs. Requests should contain a migration name (a human readable description which is matched to an actual migration pipeline), the user who kicked off the migration, and any relevant `kwargs` needed to run the migration.
- *dataflow_tasks.py*: The Celery task queues. One kicks off Apache Beam pipelines and the other monitors their status.
- *helpers.py*: Helper functions to construct the `argv` pipeline options given to each pipeline.
- *mappers.py*: The base classes underlying all our Apache Beam pipelines. One class runs namespaced migrations by flattening them into one pipeline, and another runs simpler, non-namespaced migrations.
- *migration.py*: The `Migration` object that connects human readable migration names passed from the API to actual Apache Beam pipeline functions.
- *pipelines/*: Contains all the Apache Beam pipelines for our data migrations. Put new migrations here using the old ones as examples.

### Relevant files in `migrations/`
The serializer uses two files to deal with data migration validation and post-migration work.

- *pipeline_validation.py*: For each migration, we create a validation function. That validation function is called by the serializer _before_ we send any requests to the data migration flex service. Its purpose is to catch basic `BadRequest` type errors and notifiy the Migration API user immediately. For instance, in the "move beacons" migration users must supply two `App` IDs and two `Map` IDs. Validation for that pipeline checks all four IDs are supplied and that the `App`s and `Map`s in question actually exist in the Datastore. The validation must be added to `pipeline_validation.py` _and_ the `dataflow_pipeline_validation` function in `migrations/serializer.py`.
- *post_migration.py*: For each migration (where necessary), we create a post-migration work function. This should contain the work we want to do after that migration runs. Usually we'll need to update our search docs, for instance, after we update entities in the Datstore. The deferred "poll" task in our migration serializer will kick off this post migration work once it sees the migration has run successfully (it checks the status of the `MigrationHistory` object). See `post_migration.py` for examples of how to run post-migration work.

### `__init__.py` Files and the Migration Object
In `migrations/tasks/__init__.py` and `migrations/dataflow_migrations/pipelines/__init__.py` we import the `Migration` object so we can link human-readable descriptions of each migration to the actual pipelines or map reduce tasks that run them. Those descriptions are presented in a drop-down on the `/api/migrations` endpont page. `migrations/dataflow_migrations/migration.py` must stay where it is so that both the flex service app and the standard service app can access it to populate these descriptions.

*Every time you create a new migration, you need to add it to `migrations/dataflow_migrations/pipelines/__init__.py`, using the other migrations as an example*.

### A Word on Dependencies
Setting this up was dependency hell. Our hope is that Apache Beam continues to improve their Python and Google Cloud Datastore API support, kicking their reliance on extremely outdated libraries. In the meantime, it's safe to assume `requirements.txt` has everything you need to run a pipeline. Note that the `migrations/dataflow_migrations/pipelines/` are written in terms of the Google Cloud Datastore API Apache Beam relies on (`google.cloud.proto.datastore.v1`). Apache Beam installs this API when it is installed by pip. This is a similar but different lib than the current, documented Datastore API (`google.cloud.datastore`). These two libs CANNOT be installed on the same instance. Their protobufs have the same names, which cause a ton of `protobuf message already defined` errors that grind the entire app to a screeching halt. *When you write new migration pipelines, do it in terms of the `google.cloud.proto.datastore.v1` lib*. If future versions of Apache Beam use the updated `google.cloud.datastore` API, we'll need to rewrite our migration pipelines to match, but it will make our lives easier since that API is well documented and easier to use. The `googledatastore` lib is a helper that makes several `google.cloud.proto.datastore.v1` tasks easier. It is used extensivley.

### Accessing the Flex Service
You can directly access the Docker container that is running the migrations-flex service by going to the GCP console > App Engine > Instances > select "migrations-flex" from the drop-down. Next to the listed instance (there should only be one), you'll see a drop-down labeled "SSH". Select "View gcloud command" and copy/paste the command onto your command line. Hit enter. You may be asked to set up an SSH key. Go ahead and do so. Say "y" or "yes" to all the prompts. Once you're in the flex instance, type `docker ps -a` to see the running containers. One of them should be named `gaeapp`. Type `docker exec -it gaeapp /bin/bash` to get into the container.

You'll be placed in the `/usr/src/app` directory, which contains all the data migrations code. You can type `more /var/log/supervisord/supervisord.log` to diagnose any startup issues with Celery, RabbitMQ, or Gunicorn. Celery logs live in `/var/log/celery`, RabbitMQ logs live in `/var/log/rabbitmq` and Gunicorn logs live in, you guessed it, `/var/log/gunicorn`. Errors that happen in the app before the pipeline is kicked off usually show up in `/var/log/gunicorn/gunicorn_err.log`. Errors that happen within the Celery task when the migration pipeline is kicked off should show up in `/var/log/celery/celery_tasks.log`.

Sometimes your SSH keys to access the flex instance will expire. You'll get an error saying `REMOTE HOST IDENTIFCATION HAS CHANGED`. Simply `rm ~/.ssh/google_compute*` and type in the gcloud command again. It'll then allow you to set up a new SSH key and connect.
